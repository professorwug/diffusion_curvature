[
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "from diffusion_curvature.datasets import *\nfrom diffusion_curvature.graphtools import *\nfrom diffusion_curvature.kernels import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport graphtools\n\n\ndef visualize_2_curvatures(X,k1,k2):\n    \"\"\"Makes two side-by-side 3d plots of X, the first colored by k1, the second by k2. A colorbar accompanies each.\"\"\"\n    fig = plt.figure(figsize=(12,6))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122, projection='3d')\n    ax1.scatter(X[:,0], X[:,1], X[:,2], c=k1, cmap='jet')\n    ax2.scatter(X[:,0], X[:,1], X[:,2], c=k2, cmap='jet')\n    ax1.set_title('Gaussian curvature')\n    ax2.set_title('Diffusion curvature')\n    fig.colorbar(ax1.scatter(X[:,0], X[:,1], X[:,2], c=k1, cmap='jet'), ax=ax1)\n    fig.colorbar(ax2.scatter(X[:,0], X[:,1], X[:,2], c=k2, cmap='jet'), ax=ax2)\n    plt.show()\n\nFor the hyperboloid, we found these parameters to be most impactful:\n\nanisotropy=1, when constructing the Graphtools graph. This combats some of the fluctuations in curvature caused by local density variations.\nknn=10: large enough to allow diffusion to cover the graph, local enough to capture the curvature\n\nNote that our pointcloud, unlike the ideal hyperboloid, cuts off at the top and bottom, causing diffusion to rebound and giving the impression of higher curvatures.\n\nX_hyperboloid, ks_hyperboloid = hyperboloid(3000)\nG_hyperboloid = graphtools.Graph(X_hyperboloid, knn=10, anisotropy=1)\nDC = DiffusionCurvature(t=15)\nG_hyperboloid = DC.curvature(G_hyperboloid, dimension=2)\nvisualize_2_curvatures(X_hyperboloid, ks_hyperboloid, G_hyperboloid.ks)\n\n\n\n\nAs the sphere shows,\n\nX_sphere, ks_sphere = sphere(3000)\nG_sphere = graphtools.Graph(X_sphere, knn=15, anisotropy=1)\nDC = DiffusionCurvature(t=15)\nG_sphere = DC.curvature(G_sphere,dimension=2)\nvisualize_2_curvatures(X_sphere, ks_sphere, G_sphere.ks)\n\n\n\n\n\nPure Graphs\nDiffusion curvature was designed for pointclouds, and it’s in this domain that it shows the clearest advantages over competing methods, like Ollivier Ricci curvature. But it can also be used directly on an adjacency matrix. Here’s how:"
  },
  {
    "objectID": "core (graphtools).html",
    "href": "core (graphtools).html",
    "title": "Implementation (Graphtools)",
    "section": "",
    "text": "This notebook implements diffusion curvature atop the popular Graphtools library (also maintained by the Krishnaswamy Lab). To compute the curvature of any graphtools graph, simply instantiate a DiffusionCurvature object with your choice of parameters, and pass the graphtools graph through as input.\nWhat follows is a literate implementation, showing the steps of the algorithm applied to our old friend, the torus.\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=5000)\nplot_3d(X_torus, torus_gaussian_curvature, title=\"Welcome back, old friend\")\nFirst we need to turn this into a graphtools graph. Fortunately, that’s quite simple. We’ll trust the graphtools defaults for now.\nimport graphtools\nG_torus = graphtools.Graph(X_torus, anisotropy=1,knn=10)\nG_torus_landmarked = graphtools.Graph(X_torus, n_landmark=100)\nGraphtools has some niceties, but also some limitations. It can, for instance, calculate the diffusion matrix, complete with anisotropic density normalization and automatic conversion into scipy’s sparse matrix format. It can also compute the landmark operator - a compressed version of the diffusion matrix that diffusion only between a subset of “landmark” points within a dataset - enabling us to approximately power huge diffusion matrices. All of these features will be used!\nG_torus.P\n\n&lt;2532x2532 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 34698 stored elements in Compressed Sparse Row format&gt;"
  },
  {
    "objectID": "core (graphtools).html#the-diffusion-curvature-class",
    "href": "core (graphtools).html#the-diffusion-curvature-class",
    "title": "Implementation (Graphtools)",
    "section": "The Diffusion Curvature class",
    "text": "The Diffusion Curvature class\nFollowing the convention of PyTorch modules, we separate the configuration/initialization of diffusion curvature from the operation. First, you’ll initialize the following class, then run the equivalent of a fit_transform function.\nTo better aid literate notebook-based development, the functions in the class will be attached via the fastcore @patch operator. This allows us to debug them more easily.\n\nsource\n\nDiffusionCurvature\n\n DiffusionCurvature (t:int, distance_type='PHATE', dimest=None,\n                     use_entropy:bool=False, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nint\n\nNumber of diffusion steps to use when measuring curvature. TODO: Heuristics\n\n\ndistance_type\nstr\nPHATE\n\n\n\ndimest\nNoneType\nNone\nDimensionality estimator to use. If None, defaults to KNN with default params\n\n\nuse_entropy\nbool\nFalse\nIf true, uses KL Divergence instead of Wasserstein Distances. Faster, seems empirically as good, but less proven.\n\n\nkwargs\n\n\n\n\n\n\n\nDC = DiffusionCurvature(t=8)\n\nFirst, we’ll tackle the most computationally demanding step: powering the diffusion matrix. Because graphtools defaults to scipy.sparse matrices, we can do this with the ** operation, which doesn’t (as in np arrays) perform elementwise operations. To be safe, we’ll check if there’s an np array and us np.linalg.matrix_power if so.\n\nsource\n\n\nDiffusionCurvature.power_diffusion_matrix\n\n DiffusionCurvature.power_diffusion_matrix (G:&lt;function Graph&gt;, t=None)\n\nVerifying… Does this indeed power the matrix?\n\nDC = DiffusionCurvature(t=8)\nG_torus = DC.power_diffusion_matrix(G_torus)\ndef csr_allclose(a, b, rtol=1e-4, atol = 1e-4):\n    c = np.abs(np.abs(a - b) - rtol * np.abs(b))\n    return c.max() &lt;= atol\nassert csr_allclose(G_torus.Pt, G_torus.P @ G_torus.P @ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P)\n\n\nG_torus = DC.power_diffusion_matrix(G_torus,t=20)\n\nCPU times: user 3.63 s, sys: 12.9 ms, total: 3.64 s\nWall time: 3.64 s\n\n\n\nG_torus_landmarked = DC.power_diffusion_matrix(G_torus_landmarked)\n\nWe’ll use this powered matrix to compute the manifold distances. The implementation of the distance functions can be found in the “Manifold Distances” notebook. Here, we just call them according to the class config.\n\nsource\n\n\nDiffusionCurvature.distances\n\n DiffusionCurvature.distances (G)\n\n\nG_torus = DC.distances(G_torus)\n\nTo check that all has gone according to plan, here’s an image of the torus with the distances superimposed. The scale of distances changes with the power of \\(P^t\\).\n\nplot_3d(X_torus,G_torus.D[3], \"Distances on torus\", colorbar=True)"
  },
  {
    "objectID": "core (graphtools).html#the-wasserstein-diffusion-curvature",
    "href": "core (graphtools).html#the-wasserstein-diffusion-curvature",
    "title": "Implementation (Graphtools)",
    "section": "The Wasserstein Diffusion Curvature",
    "text": "The Wasserstein Diffusion Curvature\nThis has two components: computing the spread of diffusion, and computing a “flattened facsimile” of the graph.\nFor the second, we presently give a naive implementation that presumes the dimensionality is known and constructs random noise of the same dimension and size.\n\nsource\n\nDiffusionCurvature.wasserstein_spread_of_diffusion\n\n DiffusionCurvature.wasserstein_spread_of_diffusion\n                                                     (G:graphtools.base.Da\n                                                     taGraph, idx=None)\n\nReturns how “spread out” each diffusion is, with wasserstein distance” Presumes that the manifold distances have been separately calculated If idx is passed, only computes wsd at that index\n\nDC.wasserstein_spread_of_diffusion(G_torus)\n\narray([51.06028223, 55.78248923, 55.2683009 , ..., 58.92867807,\n       68.8351783 , 56.8999532 ])\n\n\nTo create a flattened graph as similar as possible to the original, we need to use the same graphtools parameters as used to initialize G. Most of these, fortunately, are easily accessible. Some - like the type of graph - require reverse engineering.\n\nG_exact = graphtools.Graph(X_torus, graphtype='exact')\n# G_mnn = graphtools.Graph(X_torus, graphtype='mnn')\nG_knn = graphtools.Graph(X_torus, graphtype='knn')\n\n\nsource\n\n\nget_graph_type\n\n get_graph_type (G)\n\n\ntest_eq('knn', get_graph_type(G_knn))\ntest_eq('exact', get_graph_type(G_exact))\n\nThe other parameters are either stored in the\n\nG_torus.get_params()\n\n{'n_pca': None,\n 'random_state': None,\n 'kernel_symm': '+',\n 'theta': None,\n 'anisotropy': 1,\n 'knn': 10,\n 'decay': 40,\n 'bandwidth': None,\n 'bandwidth_scale': 1.0,\n 'knn_max': None,\n 'distance': 'euclidean',\n 'thresh': 0.0001,\n 'n_jobs': -1,\n 'verbose': False}\n\n\nfunction, or can be safely set to None.\n\nsource\n\n\nDiffusionCurvature.flattened_facsimile_of_graph\n\n DiffusionCurvature.flattened_facsimile_of_graph\n                                                  (G:graphtools.base.DataG\n                                                  raph, dimension)\n\nConstructs a flat graph, hewn from uniform random noise of the supplied dimension. Calculates the powered diffusion matrix on this graph.\n\nG_flattened = DC.flattened_facsimile_of_graph(G_torus,dimension=2)\n\nThis is an alternate measurement of a diffusion’s “spread”.\n\nsource\n\n\nDiffusionCurvature.entropy_of_diffusion\n\n DiffusionCurvature.entropy_of_diffusion (G:graphtools.base.DataGraph,\n                                          idx=None)\n\nReturns the pointwise entropy of diffusion from the powered diffusion matrix in the inpiut\nTreating the curvature as a signal on the graph can separate the density from the actual curvature. We can apply a low pass filter.\nAt last, we can assemble all of this into a curvature definition. The steps are: 1. Precompute \\(P^t\\) and \\(D\\). 2. Estimate the local dimension of each point, unless a dimension is given, in which case we assume it is the global dimension. 3. Construct a flattened graph for each of the distinct local dimensions. Compute the spread of the diffusion there and on the actual data. Take the difference. Huzzah - It’s diffusion curvature.\n\nsource\n\n\nDiffusionCurvature.curvature\n\n DiffusionCurvature.curvature (G:graphtools.base.DataGraph, t=None,\n                               dimension=None)\n\nComputes diffusion curvature of input graph. Stores it in G.ks\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nG\nDataGraph\n\nA graphtools graph to compute the curvature of\n\n\nt\nNoneType\nNone\nThe number of steps within the random walks. Corresponds to how local/global the curvature estimate is.\n\n\ndimension\nNoneType\nNone\nIf supplied, the intrinsic dimension of the manifold, either a list of the intrinsic dimension per point or an int of the global intrinsic dimension. If not supplied, estimates the local dimension of each point on the manifold using the estimator passed to DiffusionCurvature.\n\n\n\n\n# #|export\n# # Use graphtools to apply a low pass filter to a signal on a graph\n# @patch\n# def smooth(self:DiffusionCurvature, G, cutoff=0.1):\n    \n#     return G\n\n\nG_torus_pygsp = G_torus.to_pygsp()\n\n\nG_torus_pygsp.compute_fourier_basis()\n\n\nDC = DiffusionCurvature(t=8, dimest=None)\n\n\nG_torus = DC.curvature(G_torus) #test dimensionality estimation\n\nestimating local dimension of each point... may take a while\n\n\n\nG_torus = DC.curvature(G_torus,dimension=2)\n\n\nsource\n\n\nplot_manifold_curvature\n\n plot_manifold_curvature (G, title=None)\n\n\nplot_manifold_curvature(G_torus,\"Diffusion Curvature of the Torus\")"
  },
  {
    "objectID": "kernels.html",
    "href": "kernels.html",
    "title": "Kernels",
    "section": "",
    "text": "This notebook will establish our core utilities: functions for building the diffusion matrix, with various types of kernels."
  },
  {
    "objectID": "kernels.html#gaussian-kernel",
    "href": "kernels.html#gaussian-kernel",
    "title": "Kernels",
    "section": "Gaussian Kernel",
    "text": "Gaussian Kernel\nThis currently supports either a fixed bandwidth, which applies to all points, or an adaptive bandwidth, that creates a tailor-made bandwidth for each point.\n\nThe Median Heuristic for Kernel Bandwidth\nSetting the kernel bandwidth is one of the most important operations with any kernel method. It’s important to have a good heuristic to avoid needing to estimate this by trial and error. This function implements the median heuristic described in https://arxiv.org/pdf/1707.07269.pdf.\nThe median heuristic sets the bandwidth to \\(\\sqrt{H_n/2}\\), where \\(H_n\\) is the median of the squared distances between the upper triangle of the distance matrix.\n\nsource\n\n\nmedian_heuristic\n\n median_heuristic (D:numpy.ndarray)\n\n\n\n\n\nType\nDetails\n\n\n\n\nD\nndarray\nthe distance matrix\n\n\n\n\nsource\n\n\ngaussian_kernel\n\n gaussian_kernel (X:numpy.ndarray, kernel_type='fixed', sigma:float=0,\n                  k:float=10, anisotropic_density_normalization:float=0.5,\n                  threshold_for_small_values:float=1e-05)\n\nConstructs an affinity matrix from pointcloud data, using a gaussian kernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\npointcloud data as rows, shape n x d\n\n\nkernel_type\nstr\nfixed\neither fixed, or adaptive\n\n\nsigma\nfloat\n0\nif fixed, uses kernel bandwidth sigma. If not set, uses a heuristic to estimate a good sigma value\n\n\nk\nfloat\n10\nif adaptive, creates a different kernel bandwidth for each point, based on the distance from that point to the kth nearest neighbor\n\n\nanisotropic_density_normalization\nfloat\n0.5\nif nonzero, performs anisotropic density normalization\n\n\nthreshold_for_small_values\nfloat\n1e-05\nSets all affinities below this value to zero. Set to zero to disable."
  },
  {
    "objectID": "kernels.html#diffusion-matrix",
    "href": "kernels.html#diffusion-matrix",
    "title": "Kernels",
    "section": "Diffusion Matrix",
    "text": "Diffusion Matrix\n\nsource\n\ndiffusion_matrix\n\n diffusion_matrix (X:numpy.ndarray, kernel_type:str='fixed', sigma=0,\n                   k=10, anisotropic_density_normalization=0.5,\n                   threshold_for_small_values=1e-05)\n\nCreates a diffusion matrix from pointcloud data, by row-normalizing the affinity matrix obtained from the gaussian_kernel function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\npointcloud data\n\n\nkernel_type\nstr\nfixed\neither fixed or adaptive\n\n\nsigma\nint\n0\nif fixed, uses kernel bandwidth sigma. If not set, uses a heuristic to estimate a good sigma value\n\n\nk\nint\n10\nif adaptive, creates a different kernel bandwidth for each point, based on the distance from that point to the kth nearest neighbor\n\n\nanisotropic_density_normalization\nfloat\n0.5\nif nonzero, performs anisotropic density normalization\n\n\nthreshold_for_small_values\nfloat\n1e-05"
  },
  {
    "objectID": "kernels.html#exploration-of-different-kernel-functions",
    "href": "kernels.html#exploration-of-different-kernel-functions",
    "title": "Kernels",
    "section": "Exploration of different kernel functions",
    "text": "Exploration of different kernel functions\n\nX = np.random.rand(5,5)\nD = pairwise_distances(X)\nD\n\narray([[0.        , 0.79694242, 1.32441284, 1.0622539 , 0.8983815 ],\n       [0.79694242, 0.        , 1.05372018, 1.071993  , 0.80368695],\n       [1.32441284, 1.05372018, 0.        , 0.8524512 , 0.53872047],\n       [1.0622539 , 1.071993  , 0.8524512 , 0.        , 0.60318467],\n       [0.8983815 , 0.80368695, 0.53872047, 0.60318467, 0.        ]])\n\n\n\n# Get the distance to the kth closest neighbor\ndistance_to_k_neighbor = np.partition(D,2)[:,2]\n# [:,2] # argpartition is more efficient than argsort ([python - How to get indices of top-K values from a numpy array - Stack Overflow](https://stackoverflow.com/questions/65038206/how-to-get-indices-of-top-k-values-from-a-numpy-array))\ndistance_to_k_neighbor\n\narray([0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467])\n\n\nDouble checking that the code for the adaptive kernel works as intended:\n\ndiv2 = distance_to_k_neighbor[:,None] @ np.ones(len(D))[None,:]\ndiv2\n\narray([[0.8983815 , 0.8983815 , 0.8983815 , 0.8983815 , 0.8983815 ],\n       [0.80368695, 0.80368695, 0.80368695, 0.80368695, 0.80368695],\n       [0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 ],\n       [0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 ],\n       [0.60318467, 0.60318467, 0.60318467, 0.60318467, 0.60318467]])\n\n\n\ndiv1 = np.ones(len(D))[:,None] @ distance_to_k_neighbor[None,:]\ndiv1\n\narray([[0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467]])\n\n\nLet’s chart the torus with several different diffusion kernels.\n\nfrom diffusion_curvature.datasets import torus\n\n\nX, ks = torus(n=2000)\n\n\nX\n\narray([[-2.38963359, -0.96507252,  0.81663658],\n       [ 0.22235817, -2.66057004,  0.74250032],\n       [ 0.11289289, -1.485425  , -0.86000167],\n       ...,\n       [ 0.651027  ,  1.96528896,  0.99752496],\n       [ 1.85050078, -2.25123214, -0.40532616],\n       [-1.84763303,  1.09939814, -0.98868868]])\n\n\nTo visualize this, we’ll build a 3D plot helper, to save time in the future\n\nsource\n\nplot_3d\n\n plot_3d (X, distribution=None, title='', lim=None, use_plotly=False,\n          zlim=None, colorbar=False, cmap='plasma')\n\n\nplot_3d(X,list(range(len(X))),\"Donut with sprinkles\",colorbar = True)\n\n\n\n\nHurrah! Our donut is intact, and our plotting function is working as expected.\nNow let’s visualize some diffusions, under various kernels."
  },
  {
    "objectID": "kernels.html#the-adaptive-kernel",
    "href": "kernels.html#the-adaptive-kernel",
    "title": "Kernels",
    "section": "The Adaptive Kernel",
    "text": "The Adaptive Kernel\nHere we have the adaptive kernel born Diffusion matrix, and we visualize the diffusion centered on the point (0,-3,0), which (from the view of the plot below), should be on the outer rim of the torus, facing us directly.\n\nP = diffusion_matrix(X,kernel_type=\"adaptive\",k=20)\n\n\ndist = P[0]\nplot_3d(X,dist)"
  },
  {
    "objectID": "kernels.html#the-adaptive-anisotropic-kernel",
    "href": "kernels.html#the-adaptive-anisotropic-kernel",
    "title": "Kernels",
    "section": "The Adaptive Anisotropic Kernel",
    "text": "The Adaptive Anisotropic Kernel\nNow we’ll add one more round of density normalization with the “adaptive anisotropic” kernel: \\[ W_{a} = D^{-1} W D^{-1} \\] Where D is the matrix whose diagonals are the rowsums of W.\n\nP = diffusion_matrix(X,kernel_type=\"adaptive\",k=20,anisotropic_density_normalization=1)\n\n\ndist = P[0]\nplot_3d(X,dist)\n\n\n\n\nIt looks much the same, as expected. Ideally, this kernel will combat density related differences in the curvature, by equalizing the density.\n\nfrom diffusion_curvature.datasets import sphere\n\n\nX, ks = sphere(2000)\n\n\nA = gaussian_kernel(X,kernel_type = \"adaptive\", k = 10, anisotropic_density_normalization = 1, threshold_for_small_values=1e-5)\n\n\nA\n\narray([[0.0008864 , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.00087881, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.00113487, ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.00083949, 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.00071587,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.0009776 ]])\n\n\n\nsum(A)\n\narray([0.01780748, 0.01605671, 0.01757066, ..., 0.01677266, 0.01705182,\n       0.01707113])\n\n\n\nplot_3d(X,A[0])\n\n\n\n\n\nsource\n\ncompute_anisotropic_diffusion_matrix_from_graph\n\n compute_anisotropic_diffusion_matrix_from_graph (A:numpy.ndarray,\n                                                  alpha:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nA\nndarray\nthe adjacency/affinity matrix of the graph\n\n\nalpha\nfloat\nthe anisotropic density normalization parameter\n\n\nReturns\nndarray\n\n\n\n\n\nsource\n\n\ncompute_anisotropic_affinities_from_graph\n\n compute_anisotropic_affinities_from_graph (A:numpy.ndarray, alpha:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nA\nndarray\nthe adjacency/affinity matrix of the graph\n\n\nalpha\nfloat\nthe anisotropic density normalization parameter\n\n\nReturns\nndarray\n\n\n\n\n\n!nbdev_export"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "In this notebook, we’ll build various toy datasets and calculate their sectional curvatures.\nThis will use the Python symbolic computation library, sympy. Note: this library is not required to use the diffusion_curvature package. We merely employ it to calculate the curvature and appropriate expressions for rejection sampling.\n\nimport sympy as sym\n\n\ntheta = sym.Symbol('theta')\nphi = sym.Symbol('phi')\nR = sym.Symbol(\"R\")\nr = sym.Symbol(\"r\")\n\n\nf1 = (R + r*sym.cos(theta))*sym.cos(phi)\n\n\nsym.diff(f1,theta)\n\n\\(\\displaystyle - r \\sin{\\left(\\theta \\right)} \\cos{\\left(\\phi \\right)}\\)\n\n\n\nf = sym.Matrix([(R + r*sym.cos(theta))*sym.cos(phi), (R + r*sym.cos(theta))*sym.sin(phi), r*sym.sin(theta)])\n\n\nsym.diff(f, theta)\n\n\\(\\displaystyle \\left[\\begin{matrix}- r \\sin{\\left(\\theta \\right)} \\cos{\\left(\\phi \\right)}\\\\- r \\sin{\\left(\\phi \\right)} \\sin{\\left(\\theta \\right)}\\\\r \\cos{\\left(\\theta \\right)}\\end{matrix}\\right]\\)\n\n\n\n(sym.diff(f, theta).T  * sym.diff(f, theta))[0]\n\n\\(\\displaystyle r^{2} \\sin^{2}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} + r^{2} \\sin^{2}{\\left(\\theta \\right)} \\cos^{2}{\\left(\\phi \\right)} + r^{2} \\cos^{2}{\\left(\\theta \\right)}\\)\n\n\n\ndef rejection_sample_formula(f, variables):\n    G = sym.Matrix.zeros(2,2)\n    for i, x1 in enumerate(variables):\n        for j, x2 in enumerate(variables):\n            G[i,j] = (sym.diff(f, x1).T  * sym.diff(f, x2))[0]\n    return sym.sqrt(G.det().simplify()).simplify()\n\n\nt = rejection_sample_formula(f,[theta, phi])\nt\n\n\\(\\displaystyle \\sqrt{r^{2} \\left(R + r \\cos{\\left(\\theta \\right)}\\right)^{2}}\\)\n\n\n\nTorus\nThe curvature of the torus is given by \\[ S(\\theta) = \\frac{8 \\cos{\\theta}}{5 + \\cos{\\theta}} \\]\n\nsource\n\nrejection_sample_for_torus\n\n rejection_sample_for_torus (n, r, R)\n\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a torus. Modified from [tadasets.shapes — TaDAsets 0.1.0 documentation](https://tadasets.scikit-tda.org/en/latest/_modules/tadasets/shapes.html#torus)\nUses rejection sampling....\n  else: warn(msg)\n\nsource\n\n\ntorus\n\n torus (n=2000, c=2, a=1, noise=None, seed=None, use_guide_points=False)\n\nSample n data points on a torus. Modified from tadasets.shapes — TaDAsets 0.1.0 documentation Uses rejection sampling.\nIn addition to the randomly generated points, a few constant points have been added. The 0th point is on the outer rim, in a region of high positive curvature. The 1st point is in the inside, in a region of negative curvature, and the 2nd point is on the top, where the curvature should be closer to zero.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nNumber of data points in shape.\n\n\nc\nint\n2\nDistance from center to center of tube.\n\n\na\nint\n1\nRadius of tube.\n\n\nnoise\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\nSeed for random state.\n\n\nuse_guide_points\nbool\nFalse\n\n\n\n\nVisualize with the curvature\n\nX,ks = torus(n=5000)\nplot_3d(X, ks, title=\"Torus with scalar curvature\")\n\n\n\n\n\n\n\nOne-Sheet Hyperboloid\nFirst, let’s determine the rejection sampling formula\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\ntheta = sym.Symbol(\"theta\")\nu = sym.Symbol('u')\nf = sym.Matrix(\n    [a*sym.cos(theta)*sym.sqrt(u**2+1),b*sym.sin(theta)*sym.sqrt(u**2+1),u]\n)\n\n\nvariables = [theta, u]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{a^{2} b^{2} u^{2} + a^{2} u^{2} \\sin^{2}{\\left(\\theta \\right)} + a^{2} \\sin^{2}{\\left(\\theta \\right)} - b^{2} u^{2} \\sin^{2}{\\left(\\theta \\right)} + b^{2} u^{2} - b^{2} \\sin^{2}{\\left(\\theta \\right)} + b^{2}}\\)\n\n\n\nsource\n\nhyperboloid\n\n hyperboloid (n=2000, a=2, b=2, c=1, u_limit=2, seed=None)\n\nSample roughly n points on a hyperboloid, using rejection sampling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n2\nhyperboloid param1, by default 2\n\n\nb\nint\n2\nhyperboloid param2, by default 2\n\n\nc\nint\n1\nstretchiness in z, by default 1\n\n\nu_limit\nint\n2\nConstrain the free parameter u to [-l,l], by default 2\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_hyperboloid\n\n rejection_sample_for_hyperboloid (n, a, b, c, u_limit)\n\n&lt;function nbdev.showdoc.show_doc(sym, renderer=None, name: 'str | None' = None, title_level: 'int' = 3)&gt;\n\nX, ks = hyperboloid(2000)\nplot_3d(X,ks,colorbar=True,use_plotly=False)\n\n\n\n\n\n\n\nEllipsoid\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\nc = sym.Symbol('c')\ntheta = sym.Symbol(\"theta\")\nphi = sym.Symbol(\"phi\")\nu = sym.Symbol('u')\nf = sym.Matrix(\n    [a*sym.cos(theta)*sym.sin(phi),b*sym.sin(theta)*sym.sin(phi),c*sym.cos(phi)]\n)\n\n\nvariables = [theta, phi]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{- a^{2} b^{2} \\sin^{4}{\\left(\\phi \\right)} + a^{2} b^{2} \\sin^{2}{\\left(\\phi \\right)} + a^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} - b^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} + b^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)}}\\)\n\n\n\nsource\n\nellipsoid\n\n ellipsoid (n=2000, a=3, b=2, c=1, seed=None, noise=None)\n\nSample roughly n points on an ellipsoid, using rejection sampling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n3\nellipsoid param1, by default 3\n\n\nb\nint\n2\nellipsoid param2, by default 2\n\n\nc\nint\n1\nstretchiness in z, by default 1\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nnoise\nNoneType\nNone\n\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_ellipsoid\n\n rejection_sample_for_ellipsoid (n, a, b, c)\n\n\nX, ks = ellipsoid(n=5000, noise = 0.1)\nplot_3d(X,ks,colorbar=True)\n\n\n\n\n\n\n\nHypersphere\n\nsource\n\nsphere\n\n sphere (n, radius=1, noise=0, use_guide_points=False)\n\n\nX, ks = sphere(n=1000)\nplot_3d(X)\n\n\n\n\n\n\n\nRandom Cube\n\ndef random_cube(n):\n    \"\"\"Return a random cube\n\n    Parameters\n    ----------\n    n : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    data = np.random.rand(n,3)\n    return data\n\n\n\nSaddle Regions\nGenerate hyperbolic regions as test cases of Laziness curvature.\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\nx = sym.Symbol(\"x\")\ny = sym.Symbol(\"y\")\nf = sym.Matrix(\n    [x,y,a*x**2 + b*y**2]\n)\n\n\nvariables = [x, y]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{4 a^{2} x^{2} + 4 b^{2} y^{2} + 1}\\)\n\n\n\nsource\n\nparaboloid\n\n paraboloid (n=2000, a=1, b=-1, seed=None, use_guide_points=False)\n\nSample roughly n points on a saddle, using rejection sampling for even density coverage Defined by \\(ax^2 + by^2\\).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n1\nellipsoid param1, by default 1\n\n\nb\nint\n-1\nellipsoid param2, by default -1\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nuse_guide_points\nbool\nFalse\n\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_saddle\n\n rejection_sample_for_saddle (n, a, b)\n\n\nX, ks = paraboloid(n=10000, a = 1, b = -1,use_guide_points=True)\n\n\nplot_3d(X,ks)\n\n\n\n\n\nx = np.zeros(10)\n\n\nnp.concatenate([[0],x])\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\n\nThe Plane\n\nsource\n\nplane\n\n plane (n)\n\n\n!nbdev_export"
  },
  {
    "objectID": "manifold distances.html",
    "href": "manifold distances.html",
    "title": "Manifold Distances",
    "section": "",
    "text": "Wasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\nsource\n\n\n\n phate_distances (G:&lt;function Graph&gt;)\n\n\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=3000)\nimport graphtools\nG_torus = graphtools.Graph(X_torus)\nG_torus.Pt = G_torus.P ** 4\n\n\nG_torus = phate_distances(G_torus)\n\n\nG_torus.D\n\narray([[ 0.        , 79.44588272, 91.79222008, ..., 86.89214593,\n        76.89037532, 84.1055431 ],\n       [79.44588272,  0.        , 80.39207031, ..., 74.74836671,\n        75.23454511, 71.46405865],\n       [91.79222008, 80.39207031,  0.        , ..., 87.75808631,\n        88.17255902, 84.99987688],\n       ...,\n       [86.89214593, 74.74836671, 87.75808631, ...,  0.        ,\n        83.05921696, 79.6832004 ],\n       [76.89037532, 75.23454511, 88.17255902, ..., 83.05921696,\n         0.        , 80.13944645],\n       [84.1055431 , 71.46405865, 84.99987688, ..., 79.6832004 ,\n        80.13944645,  0.        ]])"
  },
  {
    "objectID": "manifold distances.html#phate-distances",
    "href": "manifold distances.html#phate-distances",
    "title": "Manifold Distances",
    "section": "",
    "text": "Wasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\nsource\n\n\n\n phate_distances (G:&lt;function Graph&gt;)\n\n\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=3000)\nimport graphtools\nG_torus = graphtools.Graph(X_torus)\nG_torus.Pt = G_torus.P ** 4\n\n\nG_torus = phate_distances(G_torus)\n\n\nG_torus.D\n\narray([[ 0.        , 79.44588272, 91.79222008, ..., 86.89214593,\n        76.89037532, 84.1055431 ],\n       [79.44588272,  0.        , 80.39207031, ..., 74.74836671,\n        75.23454511, 71.46405865],\n       [91.79222008, 80.39207031,  0.        , ..., 87.75808631,\n        88.17255902, 84.99987688],\n       ...,\n       [86.89214593, 74.74836671, 87.75808631, ...,  0.        ,\n        83.05921696, 79.6832004 ],\n       [76.89037532, 75.23454511, 88.17255902, ..., 83.05921696,\n         0.        , 80.13944645],\n       [84.1055431 , 71.46405865, 84.99987688, ..., 79.6832004 ,\n        80.13944645,  0.        ]])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Diffusion Curvature",
    "section": "",
    "text": "[!INFO] This code is currently in early beta. Some features, particularly those relating to dimension estimation and the construction of comparison spaces, are experimental and will likely change. Please report any issues you encounter to the Github Issues page.\nDiffusion curvature is a pointwise extension of Ollivier-Ricci curvature, designed specifically for the often messy world of pointcloud data. Its advantages include:"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Diffusion Curvature",
    "section": "Install",
    "text": "Install\n\nTo install with pip (or better yet, poetry),\npip install diffusion-curvature\nor\npoetry add diffusion-curvature\nConda releases are pending."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Diffusion Curvature",
    "section": "Usage",
    "text": "Usage\nTo compute diffusion curvature, first create a graphtools graph with your data. Graphtools offers extensive support for different kernel types (if creating from a pointcloud), and can also work with graphs in the PyGSP format. We recommend using anistropy=1, and verifying that the supplied knn value encompasses a reasonable portion of the graph.\n\nfrom diffusion_curvature.datasets import torus\nimport graphtools\nX_torus, torus_gaussian_curvature = torus(n=5000)\nG_torus = graphtools.Graph(X_torus, anisotropy=1, knn=30)\n\nGraphtools offers many additional options. For large graphs, you can speed up the powering of the diffusion matrix with landmarking: simply pass n_landmarks=1000 (e.g) when creating the graphtools graph. If you enable landmarking, diffusion-curvature will automatically use it.\nNext, instantiate a DiffusionCurvature operator.\n\nfrom diffusion_curvature.graphtools import DiffusionCurvature\nDC = DiffusionCurvature(t=12)\n\n\nsource\n\nDiffusionCurvature\n\n DiffusionCurvature (t:int, distance_type='PHATE', dimest=None,\n                     use_entropy:bool=False, **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nint\n\nNumber of diffusion steps to use when measuring curvature. TODO: Heuristics\n\n\ndistance_type\nstr\nPHATE\n\n\n\ndimest\nNoneType\nNone\nDimensionality estimator to use. If None, defaults to KNN with default params\n\n\nuse_entropy\nbool\nFalse\nIf true, uses KL Divergence instead of Wasserstein Distances. Faster, seems empirically as good, but less proven.\n\n\nkwargs\n\n\n\n\n\n\nAnd, finally, pass your graph through it. The DiffusionCurvature operator will store everything it computes – the powered diffusion matrix, the estimated manifold distances, and the curvatures – as attributes of your graph. To get the curvatures, you can run G.ks.\n\nG_torus = DC.curvature(G_torus, dimension=2) # note: this is the intrinsic dimension of the data\n\n\nplot_3d(X_torus, G_torus.ks, colorbar=True, title=\"Diffusion Curvature on the torus\")"
  },
  {
    "objectID": "core (manifoldgraph).html",
    "href": "core (manifoldgraph).html",
    "title": "Implementation (Manifold Graph)",
    "section": "",
    "text": "This notebook establishes the core of the diffusion curvature library. Our implementation is structured around the ManifoldGraph class, which stores all of the relevant information for our geometric analysis. Below this, we define various transforms on the manifold graph: 1. Powering the diffusion matrix 2. Computing Heat Geodesic Distances 3. Computing the wasserstein spread of diffusino among others.\nWhen convenient, we define each transform directly within this notebook. For some transforms, extra implementation is needed; we source those to other notebooks.\nsource"
  },
  {
    "objectID": "core (manifoldgraph).html#phate-distances",
    "href": "core (manifoldgraph).html#phate-distances",
    "title": "Implementation (Manifold Graph)",
    "section": "PHATE Distances",
    "text": "PHATE Distances\nWasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\n\nphate_distances\n\n phate_distances (x, **kwargs)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches"
  },
  {
    "objectID": "core (pyg).html",
    "href": "core (pyg).html",
    "title": "Implementation (Pytorch Geometric)",
    "section": "",
    "text": "This notebook provides a pytorch geometric implementation of the diffusion curvature algorithm."
  }
]