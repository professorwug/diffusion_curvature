[
  {
    "objectID": "3c Sampling Experiments.html",
    "href": "3c Sampling Experiments.html",
    "title": "3c Sampling Experiments on Diffusion Curvature",
    "section": "",
    "text": "::: {#cell-1 .cell 0=‘d’ 1=‘e’ 2=‘f’ 3=‘a’ 4=‘u’ 5=‘l’ 6=‘t’ 7=‘’ 8=’e’ 9=’x’ 10=’p’ 11=’ ’ 12=’3’ 13=’c’ 14=’’ 15=‘e’ 16=‘x’ 17=‘p’ 18=‘e’ 19=‘r’ 20=‘i’ 21=‘m’ 22=‘e’ 23=‘n’ 24=‘t’ 25=‘s’ 26=’ ’ 27=‘h’ 28=‘i’ 29=‘d’ 30=‘e’ execution_count=2}\n:::"
  },
  {
    "objectID": "3c Sampling Experiments.html#dimensional-analysis-of-planes",
    "href": "3c Sampling Experiments.html#dimensional-analysis-of-planes",
    "title": "3c Sampling Experiments on Diffusion Curvature",
    "section": "Dimensional Analysis of Planes",
    "text": "Dimensional Analysis of Planes\nHow does it perform with planes of varying dimensions?\nUsing clustering within the manifold:\n\nds = [3,4,5,6]\nplanes = [plane(1000*2**(d-2), d) for d in ds]\nfor i, d in enumerate(ds):\n    G = get_alpha_decay_graph(planes[i], decay=None, knn=15, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=500,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\")\n    ks = DC.curvature(G, t=8, dim=d, knn=15)\n    print(\"dimension\",d,\": Curvature of Plane is \",ks[0])\n\n\n\n\ndimension 3 : Curvature of Plane is  0.22488499\ndimension 4 : Curvature of Plane is  -0.0021333694\ndimension 5 : Curvature of Plane is  -0.1174202\ndimension 6 : Curvature of Plane is  -0.098750114\n\n\n\n\n\n\n\n\n\n\n\nWithout clustering\n\nds = [3,4,5,6]\nplanes = [plane(1000*2**(d-2), d) for d in ds]\nfor i, d in enumerate(ds):\n    G = get_alpha_decay_graph(planes[i], decay=None, knn=15, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\")\n    ks = DC.curvature(G, t=8, dim=d, knn=15)\n    print(\"dimension\",d,\": Curvature of Plane is \",ks[0])\n\n\n\n\ndimension 3 : Curvature of Plane is  -0.113577366\ndimension 4 : Curvature of Plane is  -0.031496525\ndimension 5 : Curvature of Plane is  -0.05647278\ndimension 6 : Curvature of Plane is  0.07413292\n\n\n\n\n\n\n\n\n\n\n\nConclusion: when using more points, there’s less variance between dimensions — though still a slightly alarming amount of variance within them. There doesn’t appear to be any pervasive bias induced by dimensionality in either setting. That said, this is the best possible condition, as it is literally comparing a plane to a plane."
  },
  {
    "objectID": "3c Sampling Experiments.html#planes-under-different-sampling",
    "href": "3c Sampling Experiments.html#planes-under-different-sampling",
    "title": "3c Sampling Experiments on Diffusion Curvature",
    "section": "Planes under different sampling",
    "text": "Planes under different sampling\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(1000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=15, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\")\n    ks = DC.curvature(G, t=t, dim=2, knn=15)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with No Clustering, Subtraction, t={t}\")\n\nText(0.5, 1.0, 'Ks of Plane, with No Clustering, Subtraction, t=8')\n\n\n\n\n\nThat’s not looking very good. The randomness of the plane sampling, combined with the randomness of the comparison space has created a lot of variability.\nI see two strategies to address this: using a higher \\(t\\), and comparing to a grid.\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(1000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=15, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=True)\n    ks = DC.curvature(G, t=t, dim=2, knn=15)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with Grid, No Clustering, Subtraction, t={t}\")\n\nText(0.5, 1.0, 'Ks of Plane, with Grid, No Clustering, Subtraction, t=8')\n\n\n\n\n\nNow this is odd. It appears the grid biases the results; it must have a higher-than-normal entropy, making everything appear more positive while it, the comparison, looks falsely negative. What’s up with this?\nOn a more positive note, using a grid did shave off 0.2 variance.\nHypothesis 1: It’s the kernel we’re using. That darn alpha-decay kernel is somehow changing the shape of the grid. Disabling the decay should remedy the problem. To be doubly sure that’s working, I can construct a kernel with my code.\n\nDecay=None does nothing – that’s what we were using before.\nBut perhaps it has to do with the knn value! In a grid, the 15th nearest neighbor may be further than it is on a uniformly sampled surface, because the points are arranged in squares rather than circles.\n\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(1000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=10, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=True)\n    ks = DC.curvature(G, t=t, dim=2, knn=10)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with Grid, No Clustering, Subtraction, t={t}, knn=10\")\n\nText(0.5, 1.0, 'Ks of Plane, with Grid, No Clustering, Subtraction, t=8, knn=10')\n\n\n\n\n\nSupport for the knn hypothesis. Changing k from 15 to 10 increased the perceived negativity of the grid’s curvature.\nThis would likely be best avoided by not using a knn grid; or using some average of distances, rather than the concrete distance from the kth nearest neighbor.\nIt would also, if the hypothesis is shrewd, diminish in effect the higher the k.\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(1000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=30, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=True)\n    ks = DC.curvature(G, t=t, dim=2, knn=30)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with Grid, No Clustering, Subtraction, t={t}, knn=30\")\n\nText(0.5, 1.0, 'Ks of Plane, with Grid, No Clustering, Subtraction, t=8, knn=30')\n\n\n\n\n\nIndeed, using a larger knn value decreased the descrepency considerably. But it’s still there!\nAs an ablation, here’s this same experiment on a 5000 point plane.\nHere’s the k=15 version: (With GPU, it jumps to about 10 minutes to run 1000 trials. Not bad. Thank ya, Nvidia!)\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(5000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=15, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=True)\n    ks = DC.curvature(G, t=t, dim=2, knn=15)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with Grid, No Clustering, Subtraction, t={t}, knn=15\")\n\nText(0.5, 1.0, 'Ks of Plane, with Grid, No Clustering, Subtraction, t=8')\n\n\n\n\n\n\nsampled_plane_ks = []\nt = 8\nfor i in trange(1000):\n    X_plane = plane(5000,2)\n    G = get_alpha_decay_graph(X_plane, decay=None, knn=30, anisotropy=1, )\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=True)\n    ks = DC.curvature(G, t=t, dim=2, knn=30)\n    sampled_plane_ks.append(ks[0])\n\n\n\n\n\n# plot a histogram of the curvature values\nplt.hist(sampled_plane_ks, bins=100)\nplt.title(f\"Ks of Plane, with Grid, No Clustering, Subtraction, t={t}, knn=30\")\n\nText(0.5, 1.0, 'Ks of Plane, with Grid, No Clustering, Subtraction, t=8, knn=30')\n\n\n\n\n\nThe grid, if it can be made to work, greatly reduces the variance in reported curvatures. To work with it, I see two immediate options:\n\nAdopting a non-knn kernel – something more sophisticated – that weighs across the distances of all of the k nearest points, and not merely the kth point. If used simultaneously on real data and the comparison space, this would allow us to use the grid.\nInstead of changing our kernel to match the hyper-uniform sampling of the grid, whose chief advantage is predictability, we could average the results of a large number of uniform samplings. We can take the average entropy over N uniform samplings for a grid of likely pairings between k, t, and d.\n\nI favor the latter approach, as it would also reduce the runtime of the algorithm, by precomputing the expected uniform samplings. If parameters are chosen outside of the precomputed grid, you can revert to a single sampling, as we presently do it. Additionally, this would allow fast matching of comparison spaces with graphs. At low \\(t\\) values, the diffusion entropy should be approximately equal to the flat entropy, modulo the kernel bandwidth, thus allowing us to estimate that \\(knn\\) parameter.\nThe feasibility question is this: how many pairings do we need?\n\nnum_ks = 30\nnum_ts = 50\nnum_ds = 10 # anything much higher dimensional is impossible to get enough samples from\nnum_trials = num_ks * num_ts * num_ds\nnum_trials\n\n15000\n\n\nEach of those could be done in at most 5 minutes, costing\n\nstr(num_trials/12/24)[:4] + \" days\"\n\n'52.0 days'\n\n\nOf course, I can parallelize that across Yale’s clusters, cutting it down to just a couple of days. That seems feasible.\nThe other feasibility check is whether the number of points in the comparison space changes the entropy. It shouldn’t. Let’s check:\n\nks_bigs = []\nks_smalls = []\nfor i in range(100):\n    big_plane = plane(5000,2)\n    small_plane = plane(500,2)\n    DC = DiffusionCurvature(laziness_method=\"Entropic\",points_per_cluster=None,comparison_space_size_factor=1,comparison_method=\"Subtraction\", flattening_method=\"Fixed\", use_grid=False)\n    G_big_plane = get_alpha_decay_graph(big_plane, knn=15, anisotropy=1, decay=None)\n    G_small_plane = get_alpha_decay_graph(small_plane, knn=15, anisotropy=1, decay=None)\n    ks_big = DC.unsigned_curvature(G_big_plane, t=8, idx=0)\n    ks_small = DC.unsigned_curvature(G_small_plane, t=8, idx=0)\n    ks_bigs.append(ks_big) \n    ks_smalls.append(ks_small)\n\n\n# show the mean and standard deviation of ks_bigs and ks_smalls\nprint(\"mean of ks_bigs:\", np.mean(ks_bigs))\nprint(\"mean of ks_smalls:\", np.mean(ks_smalls))\nprint(\"std of ks_bigs:\", np.std(ks_bigs))\nprint(\"std of ks_smalls:\", np.std(ks_smalls))\n\nmean of ks_bigs: 5.059733\nmean of ks_smalls: 5.0795994\nstd of ks_bigs: 0.07575969\nstd of ks_smalls: 0.0928274\n\n\nSo there is a difference arising from the number of points used. Likely this is because the diffusion, though concentrated in the center of our plane, has lots of ‘close to zero’ values that have spread across the manifold.\nI tried changing the entropy calculation to zero out elements below an epsilon threshold (set to 10e-5); this helps some, but for large discrepancies in the number of points there’s still a .02 discrepancy. Perhaps that’s close enough it can be tolerated – it’s certainly less than the variance within different uniform samplings of the plane.\nThe next step is to modify the ‘Fixed’ comparison space construction to first load a database of flat entropies (on initialization of the class?), check if the current parameters are within the database, and, if not, average the uniform sampling N times."
  }
]