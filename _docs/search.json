[
  {
    "objectID": "manifold distances.html",
    "href": "manifold distances.html",
    "title": "Manifold Distances",
    "section": "",
    "text": "Wasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\nsource\n\n\n\n phate_distances (G:&lt;function Graph&gt;)\n\n\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=3000)\nimport graphtools\nG_torus = graphtools.Graph(X_torus)\nG_torus.Pt = G_torus.P ** 4\n\n\nG_torus = phate_distances(G_torus)\n\n\nG_torus.D\n\narray([[ 0.        , 79.44588272, 91.79222008, ..., 86.89214593,\n        76.89037532, 84.1055431 ],\n       [79.44588272,  0.        , 80.39207031, ..., 74.74836671,\n        75.23454511, 71.46405865],\n       [91.79222008, 80.39207031,  0.        , ..., 87.75808631,\n        88.17255902, 84.99987688],\n       ...,\n       [86.89214593, 74.74836671, 87.75808631, ...,  0.        ,\n        83.05921696, 79.6832004 ],\n       [76.89037532, 75.23454511, 88.17255902, ..., 83.05921696,\n         0.        , 80.13944645],\n       [84.1055431 , 71.46405865, 84.99987688, ..., 79.6832004 ,\n        80.13944645,  0.        ]])"
  },
  {
    "objectID": "manifold distances.html#phate-distances",
    "href": "manifold distances.html#phate-distances",
    "title": "Manifold Distances",
    "section": "",
    "text": "Wasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\nsource\n\n\n\n phate_distances (G:&lt;function Graph&gt;)\n\n\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=3000)\nimport graphtools\nG_torus = graphtools.Graph(X_torus)\nG_torus.Pt = G_torus.P ** 4\n\n\nG_torus = phate_distances(G_torus)\n\n\nG_torus.D\n\narray([[ 0.        , 79.44588272, 91.79222008, ..., 86.89214593,\n        76.89037532, 84.1055431 ],\n       [79.44588272,  0.        , 80.39207031, ..., 74.74836671,\n        75.23454511, 71.46405865],\n       [91.79222008, 80.39207031,  0.        , ..., 87.75808631,\n        88.17255902, 84.99987688],\n       ...,\n       [86.89214593, 74.74836671, 87.75808631, ...,  0.        ,\n        83.05921696, 79.6832004 ],\n       [76.89037532, 75.23454511, 88.17255902, ..., 83.05921696,\n         0.        , 80.13944645],\n       [84.1055431 , 71.46405865, 84.99987688, ..., 79.6832004 ,\n        80.13944645,  0.        ]])"
  },
  {
    "objectID": "core (pyg).html",
    "href": "core (pyg).html",
    "title": "Diffusion Curvature Core (Pyg)",
    "section": "",
    "text": "from diffusion_curvature.core import *\nfrom fastcore.all import *\nThis notebook establishes the core of the diffusion curvature library. Our implementation is structured around the ManifoldGraph class, which stores all of the relevant information for our geometric analysis. Below this, we define various transforms on the manifold graph: 1. Powering the diffusion matrix 2. Computing Heat Geodesic Distances 3. Computing the wasserstein spread of diffusino among others.\nWhen convenient, we define each transform directly within this notebook. For some transforms, extra implementation is needed; we source those to other notebooks.\nTODOs: - Make the ManifoldGraph class a subclass of torch_geometric’s data. - Transition from numpy to torch\nIn this implementation, we define ManifoldGraph as a subclass of Pytorch Geometric’s standard graph object. This has a number of pleasant benefits: - It scales to much larger graphs: PyG uses a native sparse format. This allows diffusing over thousands of nodes, without any loss of fidelity. - It inherits gpu compatibility\nfrom fastcore.all import *\nfrom diffusion_curvature.core import *\nimport torch_geometric\nclass ManifoldGraph(torch_geometric.data.Data):\n    \"\"\"\n    Class for Manifold Graphs. Stores the basic graph information used for geometric analysis: \n    A (the affinity/adjacency matrix), P (the diffusion matrix), P^t (the powered diffusion matrix), and D (the manifold distance matrix)\n    Takes raw points, X. Can also optionally pass in any of the above to save the trouble of recomputing it.\n    \"\"\"\n    def __init__(self, \n    coords=None, # raw points\n    dimension=None, # dimension of manifold. \n    kernel_type:str = \"adaptive\", # \"fixed\" or \"adaptive\"\n    anisotropic_density_normalization:float=0.5,  # normalize out this amount of the density. 0 is none. 1 is all.\n    num_neighbors = 10, # for adaptive kernel, constructs graph to connect this number of neighbors\n    t = 10, # steps of diffusion\n    A = None, \n    P = None, \n    Pt = None, \n    D = None, \n    *kw_args\n    ):\n        \"\"\"\n        We compute the affinity matrix and diffusion matrix automatically\n        \"\"\"\n        self.node_attributes = {}\n        store_attr()\n        if not self.A:\n            self.A = gaussian_kernel(self.X, kernel_type=self.kernel_type, k = self.num_neighbors, anisotropic_density_normalization=self.anisotropic_density_normalization)\n            np.fill_diagonal(self.A,0)\n            \n        if not self.P:\n            self.P = compute_anisotropic_diffusion_matrix_from_graph(self.A, self.anisotropic_density_normalization)        \n    def num_nodes(self):\n        return len(self.A)\nimport numpy as np\ndef power_diffusion_matrix(G:ManifoldGraph, t:int=None):\n    # Raises the diffusion matrix to t\n    if not t:\n        t = G.t\n    G.Pt = np.linalg.matrix_power(G.P, t)\n    return G"
  },
  {
    "objectID": "core (pyg).html#phate-distances",
    "href": "core (pyg).html#phate-distances",
    "title": "Diffusion Curvature Core (Pyg)",
    "section": "PHATE Distances",
    "text": "PHATE Distances\nWasserstein Diffusion Curvature – despite the name – requires only manifold distances. This saves quite a bit of computation, but doesn’t alleviate the need for a good approximation of the manifold’s geodesic distance. Here, we implement one straightforward and accurate manifold distance: that proposed by Moon et al. in PHATE (2019). The PHATE distance is an extension of the diffusion distance, except instead of calculating the L2 distances between diffusion coordinates (which corresponds roughly to the rows of the diffusion matrix), it takes the L2 distances between the log-transformed diffusions. This flips the weighting from local to global, as a diffusion that assigns a small mass where another assigns a miniscule mass becomes much further than those that differ only at their centers. This log transform has the additional advantage of, through the WAWA formulation of the heat equation, recovering the distance term.\nIt is defined as: \\[d_p(x,y) = \\| \\log(p_y^t)-\\log(p_x^t) \\|_2 \\]\n\nfrom sklearn.metrics import pairwise_distances\nimport numpy as np\n@Transform\ndef phate_distances(G:ManifoldGraph):\n    assert G.Pt is not None\n    log_Pts = -np.log(G.Pt + 1e-6)\n    D = pairwise_distances(log_Pts)\n    G.D = D\n    return G"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "foo\n\n foo ()"
  },
  {
    "objectID": "core (graphtools).html",
    "href": "core (graphtools).html",
    "title": "Diffusion Curvature - Graphtools Implementation",
    "section": "",
    "text": "This notebook implements diffusion curvature atop the popular Graphtools library (also maintained by the Krishnaswamy Lab). To compute the curvature of any graphtools graph, simply instantiate a DiffusionCurvature object with your choice of parameters, and pass the graphtools graph through as input.\nWhat follows is a literate implementation, showing the steps of the algorithm applied to our old friend, the torus.\nfrom diffusion_curvature.datasets import torus\nX_torus, torus_gaussian_curvature = torus(n=5000)\nplot_3d(X_torus, torus_gaussian_curvature, title=\"Welcome back, old friend\")\nFirst we need to turn this into a graphtools graph. Fortunately, that’s quite simple. We’ll trust the graphtools defaults for now.\nimport graphtools\nG_torus = graphtools.Graph(X_torus, anisotropy=1,)\nG_torus_landmarked = graphtools.Graph(X_torus, n_landmark=100)\nGraphtools has some niceties, but also some limitations. It can, for instance, calculate the diffusion matrix, complete with anisotropic density normalization and automatic conversion into scipy’s sparse matrix format. It can also compute the landmark operator - a compressed version of the diffusion matrix that diffusion only between a subset of “landmark” points within a dataset - enabling us to approximately power huge diffusion matrices. All of these features will be used!\nG_torus.P\n\n&lt;2513x2513 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 19369 stored elements in Compressed Sparse Row format&gt;"
  },
  {
    "objectID": "core (graphtools).html#the-diffusion-curvature-class",
    "href": "core (graphtools).html#the-diffusion-curvature-class",
    "title": "Diffusion Curvature - Graphtools Implementation",
    "section": "The Diffusion Curvature class",
    "text": "The Diffusion Curvature class\nFollowing the convention of PyTorch modules, we separate the configuration/initialization of diffusion curvature from the operation. First, you’ll initialize the following class, then run the equivalent of a fit_transform function.\nTo better aid literate notebook-based development, the functions in the class will be attached via the fastcore @patch operator. This allows us to debug them more easily.\n\nsource\n\nDiffusionCurvature\n\n DiffusionCurvature (t:int, distance_type='PHATE', use_entropy:bool=False,\n                     **kwargs)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\nint\n\nNumber of diffusion steps to use when measuring curvature. TODO: Heuristics\n\n\ndistance_type\nstr\nPHATE\n\n\n\nuse_entropy\nbool\nFalse\nIf true, uses KL Divergence instead of Wasserstein Distances. Faster, seems empirically as good, but less proven.\n\n\nkwargs\n\n\n\n\n\n\n\nDC = DiffusionCurvature(t=8)\n\nFirst, we’ll tackle the most computationally demanding step: powering the diffusion matrix. Because graphtools defaults to scipy.sparse matrices, we can do this with the ** operation, which doesn’t (as in np arrays) perform elementwise operations. To be safe, we’ll check if there’s an np array and us np.linalg.matrix_power if so.\n\nsource\n\n\nDiffusionCurvature.power_diffusion_matrix\n\n DiffusionCurvature.power_diffusion_matrix (G:&lt;function Graph&gt;, t=None)\n\nVerifying… Does this indeed power the matrix?\n\nDC = DiffusionCurvature(t=8)\nG_torus = DC.power_diffusion_matrix(G_torus)\ndef csr_allclose(a, b, rtol=1e-4, atol = 1e-4):\n    c = np.abs(np.abs(a - b) - rtol * np.abs(b))\n    return c.max() &lt;= atol\nassert csr_allclose(G_torus.Pt, G_torus.P @ G_torus.P @ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P@ G_torus.P)\n\nWe’ll use this powered matrix to compute the manifold distances. The implementation of the distance functions can be found in the “Manifold Distances” notebook. Here, we just call them according to the class config.\n\nsource\n\n\nDiffusionCurvature.distances\n\n DiffusionCurvature.distances (G)\n\n\nG_torus = DC.distances(G_torus)\n\nTo check that all has gone according to plan, here’s an image of the torus with the distances superimposed. The scale of distances changes with the power of \\(P^t\\).\n\nplot_3d(X_torus,G_torus.D[3], \"Distances on torus\", colorbar=True)"
  },
  {
    "objectID": "core (graphtools).html#the-wasserstein-diffusion-curvature",
    "href": "core (graphtools).html#the-wasserstein-diffusion-curvature",
    "title": "Diffusion Curvature - Graphtools Implementation",
    "section": "The Wasserstein Diffusion Curvature",
    "text": "The Wasserstein Diffusion Curvature\nThis has two components: computing the spread of diffusion, and computing a “flattened facsimile” of the graph.\nFor the second, we presently give a naive implementation that presumes the dimensionality is known and constructs random noise of the same dimension and size.\n\nsource\n\nDiffusionCurvature.wasserstein_spread_of_diffusion\n\n DiffusionCurvature.wasserstein_spread_of_diffusion\n                                                     (G:graphtools.base.Da\n                                                     taGraph, idx=None)\n\nReturns how “spread out” each diffusion is, with wasserstein distance” Presumes that the manifold distances have been separately calculated If idx is passed, only computes wsd at that index\n\nDC.wasserstein_spread_of_diffusion(G_torus)\n\narray([30.88351409, 24.32500363, 23.41811702, ..., 25.66057601,\n       30.80773335, 21.38009235])\n\n\nTo create a flattened graph as similar as possible to the original, we need to use the same graphtools parameters as used to initialize G. Most of these, fortunately, are easily accessible. Some - like the type of graph - require reverse engineering.\n\nG_exact = graphtools.Graph(X_torus, graphtype='exact')\n# G_mnn = graphtools.Graph(X_torus, graphtype='mnn')\nG_knn = graphtools.Graph(X_torus, graphtype='knn')\n\n\nsource\n\n\nget_graph_type\n\n get_graph_type (G)\n\n\ntest_eq('knn', get_graph_type(G_knn))\ntest_eq('exact', get_graph_type(G_exact))\n\nThe other parameters are either stored in the\n\nG_torus.get_params()\n\n{'n_pca': None,\n 'random_state': None,\n 'kernel_symm': '+',\n 'theta': None,\n 'anisotropy': 1,\n 'knn': 5,\n 'decay': 40,\n 'bandwidth': None,\n 'bandwidth_scale': 1.0,\n 'knn_max': None,\n 'distance': 'euclidean',\n 'thresh': 0.0001,\n 'n_jobs': -1,\n 'verbose': False}\n\n\nfunction, or can be safely set to None.\n\nsource\n\n\nDiffusionCurvature.flattened_facsimile_of_graph\n\n DiffusionCurvature.flattened_facsimile_of_graph\n                                                  (G:graphtools.base.DataG\n                                                  raph, dimension)\n\nConstructs a flat graph, hewn from uniform random noise of the supplied dimension. Calculates the powered diffusion matrix on this graph.\n\nG_flattened = DC.flattened_facsimile_of_graph(G_torus,dimension=2)\n\nThis is an alternate measurement of a diffusion’s “spread”.\n\nsource\n\n\nDiffusionCurvature.entropy_of_diffusion\n\n DiffusionCurvature.entropy_of_diffusion (G:graphtools.base.DataGraph,\n                                          idx=None)\n\nReturns the pointwise entropy of diffusion from the powered diffusion matrix in the inpiut\nAt last, we can assemble all of this into a curvature definition. The steps are: 1. Precompute \\(P^t\\) and \\(D\\). 2. Estimate the local dimension of each point, unless a dimension is given, in which case we assume it is the global dimension. 3. Construct a flattened graph for each of the distinct local dimensions. Compute the spread of the diffusion there and on the actual data. Take the difference. Huzzah - It’s diffusion curvature.\n\nsource\n\n\nDiffusionCurvature.curvature\n\n DiffusionCurvature.curvature (G:graphtools.base.DataGraph, t=None,\n                               dimension:int=None)\n\nComputes diffusion curvature of input graph. Stores it in G.ks\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nG\nDataGraph\n\nA graphtools graph to compute the curvature of\n\n\nt\nNoneType\nNone\nThe number of steps within the random walks. Corresponds to how local/global the curvature estimate is.\n\n\ndimension\nint\nNone\nIf supplied, the global manifold dimension. If not supplied, estimates the local dimension of each point.\n\n\n\n\nG_torus.D[0]\n\narray([  0.        , 112.13655477, 111.98548138, ..., 114.4421877 ,\n       117.69232473, 111.46452324])\n\n\n\nG_torus = DC.curvature(G_torus,dimension=2)\n\n\nsource\n\n\nplot_manifold_curvature\n\n plot_manifold_curvature (G, title=None)\n\n\nplot_manifold_curvature(G_torus,\"Diffusion Curvature of the Torus\")"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Torus",
    "section": "",
    "text": "In this notebook, we’ll build various toy datasets and calculate their sectional curvatures.\nThis will use the Python symbolic computation library, sympy. Note: this library is not required to use the diffusion_curvature package. We merely employ it to calculate the curvature and appropriate expressions for rejection sampling.\n\nimport sympy as sym\n\n\ntheta = sym.Symbol('theta')\nphi = sym.Symbol('phi')\nR = sym.Symbol(\"R\")\nr = sym.Symbol(\"r\")\n\n\nf1 = (R + r*sym.cos(theta))*sym.cos(phi)\n\n\nsym.diff(f1,theta)\n\n\\(\\displaystyle - r \\sin{\\left(\\theta \\right)} \\cos{\\left(\\phi \\right)}\\)\n\n\n\nf = sym.Matrix([(R + r*sym.cos(theta))*sym.cos(phi), (R + r*sym.cos(theta))*sym.sin(phi), r*sym.sin(theta)])\n\n\nsym.diff(f, theta)\n\n\\(\\displaystyle \\left[\\begin{matrix}- r \\sin{\\left(\\theta \\right)} \\cos{\\left(\\phi \\right)}\\\\- r \\sin{\\left(\\phi \\right)} \\sin{\\left(\\theta \\right)}\\\\r \\cos{\\left(\\theta \\right)}\\end{matrix}\\right]\\)\n\n\n\n(sym.diff(f, theta).T  * sym.diff(f, theta))[0]\n\n\\(\\displaystyle r^{2} \\sin^{2}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} + r^{2} \\sin^{2}{\\left(\\theta \\right)} \\cos^{2}{\\left(\\phi \\right)} + r^{2} \\cos^{2}{\\left(\\theta \\right)}\\)\n\n\n\ndef rejection_sample_formula(f, variables):\n    G = sym.Matrix.zeros(2,2)\n    for i, x1 in enumerate(variables):\n        for j, x2 in enumerate(variables):\n            G[i,j] = (sym.diff(f, x1).T  * sym.diff(f, x2))[0]\n    return sym.sqrt(G.det().simplify()).simplify()\n\n\nt = rejection_sample_formula(f,[theta, phi])\nt\n\n\\(\\displaystyle \\sqrt{r^{2} \\left(R + r \\cos{\\left(\\theta \\right)}\\right)^{2}}\\)\n\n\nThe curvature of the torus is given by \\[ S(\\theta) = \\frac{8 \\cos{\\theta}}{5 + \\cos{\\theta}} \\]\n\nsource\n\nrejection_sample_for_torus\n\n rejection_sample_for_torus (n, r, R)\n\n/home/piriac/mambaforge/envs/diffusion_curvature/lib/python3.11/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a torus. Modified from [tadasets.shapes — TaDAsets 0.1.0 documentation](https://tadasets.scikit-tda.org/en/latest/_modules/tadasets/shapes.html#torus)\nUses rejection sampling....\n  else: warn(msg)\n\nsource\n\n\ntorus\n\n torus (n=2000, c=2, a=1, noise=None, seed=None, use_guide_points=False)\n\nSample n data points on a torus. Modified from tadasets.shapes — TaDAsets 0.1.0 documentation Uses rejection sampling.\nIn addition to the randomly generated points, a few constant points have been added. The 0th point is on the outer rim, in a region of high positive curvature. The 1st point is in the inside, in a region of negative curvature, and the 2nd point is on the top, where the curvature should be closer to zero.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nNumber of data points in shape.\n\n\nc\nint\n2\nDistance from center to center of tube.\n\n\na\nint\n1\nRadius of tube.\n\n\nnoise\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\nSeed for random state.\n\n\nuse_guide_points\nbool\nFalse\n\n\n\n\nVisualize with the curvature\n\nX,ks = torus(n=5000)\nplot_3d(X, ks, title=\"Torus with scalar curvature\")\n\n\n\n\n\n\nOne-Sheet Hyperboloid\nFirst, let’s determine the rejection sampling formula\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\ntheta = sym.Symbol(\"theta\")\nu = sym.Symbol('u')\nf = sym.Matrix(\n    [a*sym.cos(theta)*sym.sqrt(u**2+1),b*sym.sin(theta)*sym.sqrt(u**2+1),u]\n)\n\n\nvariables = [theta, u]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{a^{2} b^{2} u^{2} + a^{2} u^{2} \\sin^{2}{\\left(\\theta \\right)} + a^{2} \\sin^{2}{\\left(\\theta \\right)} - b^{2} u^{2} \\sin^{2}{\\left(\\theta \\right)} + b^{2} u^{2} - b^{2} \\sin^{2}{\\left(\\theta \\right)} + b^{2}}\\)\n\n\n\nsource\n\nhyperboloid\n\n hyperboloid (n=2000, a=2, b=2, c=1, u_limit=2, seed=None)\n\nSample roughly n points on a hyperboloid, using rejection sampling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n2\nhyperboloid param1, by default 2\n\n\nb\nint\n2\nhyperboloid param2, by default 2\n\n\nc\nint\n1\nstretchiness in z, by default 1\n\n\nu_limit\nint\n2\nConstrain the free parameter u to [-l,l], by default 2\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_hyperboloid\n\n rejection_sample_for_hyperboloid (n, a, b, c, u_limit)\n\n&lt;function nbdev.showdoc.show_doc(sym, renderer=None, name: 'str | None' = None, title_level: 'int' = 3)&gt;\n\nX, ks = hyperboloid(2000)\nplot_3d(X,ks,colorbar=True,use_plotly=False)\n\n\n\n\n\n\n\nEllipsoid\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\nc = sym.Symbol('c')\ntheta = sym.Symbol(\"theta\")\nphi = sym.Symbol(\"phi\")\nu = sym.Symbol('u')\nf = sym.Matrix(\n    [a*sym.cos(theta)*sym.sin(phi),b*sym.sin(theta)*sym.sin(phi),c*sym.cos(phi)]\n)\n\n\nvariables = [theta, phi]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{- a^{2} b^{2} \\sin^{4}{\\left(\\phi \\right)} + a^{2} b^{2} \\sin^{2}{\\left(\\phi \\right)} + a^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} - b^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)} \\sin^{2}{\\left(\\theta \\right)} + b^{2} c^{2} \\sin^{4}{\\left(\\phi \\right)}}\\)\n\n\n\nsource\n\nellipsoid\n\n ellipsoid (n=2000, a=3, b=2, c=1, seed=None, noise=None)\n\nSample roughly n points on an ellipsoid, using rejection sampling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n3\nellipsoid param1, by default 3\n\n\nb\nint\n2\nellipsoid param2, by default 2\n\n\nc\nint\n1\nstretchiness in z, by default 1\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nnoise\nNoneType\nNone\n\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_ellipsoid\n\n rejection_sample_for_ellipsoid (n, a, b, c)\n\n\nX, ks = ellipsoid(n=5000, noise = 0.1)\nplot_3d(X,ks,colorbar=True)\n\n\n\n\n\n\n\nHypersphere\n\nsource\n\nsphere\n\n sphere (n, radius=1, noise=0, use_guide_points=False)\n\n\nX, ks = sphere(n=1000)\nplot_3d(X)\n\n\n\n\n\n\n\nRandom Cube\n\ndef random_cube(n):\n    \"\"\"Return a random cube\n\n    Parameters\n    ----------\n    n : _type_\n        _description_\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    data = np.random.rand(n,3)\n    return data\n\n\n\nSaddle Regions\nGenerate hyperbolic regions as test cases of Laziness curvature.\n\na = sym.Symbol('a')\nb = sym.Symbol('b')\nx = sym.Symbol(\"x\")\ny = sym.Symbol(\"y\")\nf = sym.Matrix(\n    [x,y,a*x**2 + b*y**2]\n)\n\n\nvariables = [x, y]\nrej = rejection_sample_formula(f, variables)\nrej\n\n\\(\\displaystyle \\sqrt{4 a^{2} x^{2} + 4 b^{2} y^{2} + 1}\\)\n\n\n\nsource\n\nparaboloid\n\n paraboloid (n=2000, a=1, b=-1, seed=None, use_guide_points=False)\n\nSample roughly n points on a saddle, using rejection sampling for even density coverage Defined by \\(ax^2 + by^2\\).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n2000\nnumber of points, by default 2000\n\n\na\nint\n1\nellipsoid param1, by default 1\n\n\nb\nint\n-1\nellipsoid param2, by default -1\n\n\nseed\nNoneType\nNone\nFor repeatability, seed the randomness, by default None\n\n\nuse_guide_points\nbool\nFalse\n\n\n\nReturns\nThe sampled points, and the curvatures of each point\n\n\n\n\n\n\nsource\n\n\nrejection_sample_for_saddle\n\n rejection_sample_for_saddle (n, a, b)\n\n\nX, ks = paraboloid(n=10000, a = 1, b = -1,use_guide_points=True)\n\n\nplot_3d(X,ks)\n\n\n\n\n\nx = np.zeros(10)\n\n\nnp.concatenate([[0],x])\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\nnp.savetxt(\"datasets/hyperboloid.txt\",X)\n\n\nY = np.loadtxt(\"datasets/hyperboloid.txt\",dtype=float)\n\n\nY\n\narray([[ 0.        ,  0.        ,  0.        ],\n       [-0.9521675 ,  0.29979808,  0.81674406],\n       [-0.77266581, -0.28437732,  0.51614199],\n       ...,\n       [ 0.00402681,  0.94392053, -0.89096976],\n       [-0.42648474, -0.18276849,  0.14848491],\n       [-0.64865911,  0.02571754,  0.42009725]])\n\n\n\n\n\nThe Plane\n\nsource\n\nplane\n\n plane (n)\n\n\n!nbdev_export"
  },
  {
    "objectID": "kernels.html",
    "href": "kernels.html",
    "title": "Kernels",
    "section": "",
    "text": "This notebook will establish our core utilities: functions for building the diffusion matrix, with various types of kernels."
  },
  {
    "objectID": "kernels.html#gaussian-kernel",
    "href": "kernels.html#gaussian-kernel",
    "title": "Kernels",
    "section": "Gaussian Kernel",
    "text": "Gaussian Kernel\nThis currently supports either a fixed bandwidth, which applies to all points, or an adaptive bandwidth, that creates a tailor-made bandwidth for each point.\n\nThe Median Heuristic for Kernel Bandwidth\nSetting the kernel bandwidth is one of the most important operations with any kernel method. It’s important to have a good heuristic to avoid needing to estimate this by trial and error. This function implements the median heuristic described in https://arxiv.org/pdf/1707.07269.pdf.\nThe median heuristic sets the bandwidth to \\(\\sqrt{H_n/2}\\), where \\(H_n\\) is the median of the squared distances between the upper triangle of the distance matrix.\n\nsource\n\n\nmedian_heuristic\n\n median_heuristic (D:numpy.ndarray)\n\n\n\n\n\nType\nDetails\n\n\n\n\nD\nndarray\nthe distance matrix\n\n\n\n\nsource\n\n\ngaussian_kernel\n\n gaussian_kernel (X:numpy.ndarray, kernel_type='fixed', sigma:float=0,\n                  k:float=10, anisotropic_density_normalization:float=0.5,\n                  threshold_for_small_values:float=1e-05)\n\nConstructs an affinity matrix from pointcloud data, using a gaussian kernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\npointcloud data as rows, shape n x d\n\n\nkernel_type\nstr\nfixed\neither fixed, or adaptive\n\n\nsigma\nfloat\n0\nif fixed, uses kernel bandwidth sigma. If not set, uses a heuristic to estimate a good sigma value\n\n\nk\nfloat\n10\nif adaptive, creates a different kernel bandwidth for each point, based on the distance from that point to the kth nearest neighbor\n\n\nanisotropic_density_normalization\nfloat\n0.5\nif nonzero, performs anisotropic density normalization\n\n\nthreshold_for_small_values\nfloat\n1e-05\nSets all affinities below this value to zero. Set to zero to disable."
  },
  {
    "objectID": "kernels.html#diffusion-matrix",
    "href": "kernels.html#diffusion-matrix",
    "title": "Kernels",
    "section": "Diffusion Matrix",
    "text": "Diffusion Matrix\n\nsource\n\ndiffusion_matrix\n\n diffusion_matrix (X:numpy.ndarray, kernel_type:str='fixed', sigma=0,\n                   k=10, anisotropic_density_normalization=0.5,\n                   threshold_for_small_values=1e-05)\n\nCreates a diffusion matrix from pointcloud data, by row-normalizing the affinity matrix obtained from the gaussian_kernel function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\npointcloud data\n\n\nkernel_type\nstr\nfixed\neither fixed or adaptive\n\n\nsigma\nint\n0\nif fixed, uses kernel bandwidth sigma. If not set, uses a heuristic to estimate a good sigma value\n\n\nk\nint\n10\nif adaptive, creates a different kernel bandwidth for each point, based on the distance from that point to the kth nearest neighbor\n\n\nanisotropic_density_normalization\nfloat\n0.5\nif nonzero, performs anisotropic density normalization\n\n\nthreshold_for_small_values\nfloat\n1e-05"
  },
  {
    "objectID": "kernels.html#exploration-of-different-kernel-functions",
    "href": "kernels.html#exploration-of-different-kernel-functions",
    "title": "Kernels",
    "section": "Exploration of different kernel functions",
    "text": "Exploration of different kernel functions\n\nX = np.random.rand(5,5)\nD = pairwise_distances(X)\nD\n\narray([[0.        , 0.79694242, 1.32441284, 1.0622539 , 0.8983815 ],\n       [0.79694242, 0.        , 1.05372018, 1.071993  , 0.80368695],\n       [1.32441284, 1.05372018, 0.        , 0.8524512 , 0.53872047],\n       [1.0622539 , 1.071993  , 0.8524512 , 0.        , 0.60318467],\n       [0.8983815 , 0.80368695, 0.53872047, 0.60318467, 0.        ]])\n\n\n\n# Get the distance to the kth closest neighbor\ndistance_to_k_neighbor = np.partition(D,2)[:,2]\n# [:,2] # argpartition is more efficient than argsort ([python - How to get indices of top-K values from a numpy array - Stack Overflow](https://stackoverflow.com/questions/65038206/how-to-get-indices-of-top-k-values-from-a-numpy-array))\ndistance_to_k_neighbor\n\narray([0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467])\n\n\nDouble checking that the code for the adaptive kernel works as intended:\n\ndiv2 = distance_to_k_neighbor[:,None] @ np.ones(len(D))[None,:]\ndiv2\n\narray([[0.8983815 , 0.8983815 , 0.8983815 , 0.8983815 , 0.8983815 ],\n       [0.80368695, 0.80368695, 0.80368695, 0.80368695, 0.80368695],\n       [0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 ],\n       [0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 , 0.8524512 ],\n       [0.60318467, 0.60318467, 0.60318467, 0.60318467, 0.60318467]])\n\n\n\ndiv1 = np.ones(len(D))[:,None] @ distance_to_k_neighbor[None,:]\ndiv1\n\narray([[0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467],\n       [0.8983815 , 0.80368695, 0.8524512 , 0.8524512 , 0.60318467]])\n\n\nLet’s chart the torus with several different diffusion kernels.\n\nfrom diffusion_curvature.datasets import torus\n\n\nX, ks = torus(n=2000)\n\n\nX\n\narray([[-2.38963359, -0.96507252,  0.81663658],\n       [ 0.22235817, -2.66057004,  0.74250032],\n       [ 0.11289289, -1.485425  , -0.86000167],\n       ...,\n       [ 0.651027  ,  1.96528896,  0.99752496],\n       [ 1.85050078, -2.25123214, -0.40532616],\n       [-1.84763303,  1.09939814, -0.98868868]])\n\n\nTo visualize this, we’ll build a 3D plot helper, to save time in the future\n\nsource\n\nplot_3d\n\n plot_3d (X, distribution=None, title='', lim=None, use_plotly=False,\n          zlim=None, colorbar=False, cmap='plasma')\n\n\nplot_3d(X,list(range(len(X))),\"Donut with sprinkles\",colorbar = True)\n\n\n\n\nHurrah! Our donut is intact, and our plotting function is working as expected.\nNow let’s visualize some diffusions, under various kernels."
  },
  {
    "objectID": "kernels.html#the-adaptive-kernel",
    "href": "kernels.html#the-adaptive-kernel",
    "title": "Kernels",
    "section": "The Adaptive Kernel",
    "text": "The Adaptive Kernel\nHere we have the adaptive kernel born Diffusion matrix, and we visualize the diffusion centered on the point (0,-3,0), which (from the view of the plot below), should be on the outer rim of the torus, facing us directly.\n\nP = diffusion_matrix(X,kernel_type=\"adaptive\",k=20)\n\n\ndist = P[0]\nplot_3d(X,dist)"
  },
  {
    "objectID": "kernels.html#the-adaptive-anisotropic-kernel",
    "href": "kernels.html#the-adaptive-anisotropic-kernel",
    "title": "Kernels",
    "section": "The Adaptive Anisotropic Kernel",
    "text": "The Adaptive Anisotropic Kernel\nNow we’ll add one more round of density normalization with the “adaptive anisotropic” kernel: \\[ W_{a} = D^{-1} W D^{-1} \\] Where D is the matrix whose diagonals are the rowsums of W.\n\nP = diffusion_matrix(X,kernel_type=\"adaptive\",k=20,anisotropic_density_normalization=1)\n\n\ndist = P[0]\nplot_3d(X,dist)\n\n\n\n\nIt looks much the same, as expected. Ideally, this kernel will combat density related differences in the curvature, by equalizing the density.\n\nfrom diffusion_curvature.datasets import sphere\n\n\nX, ks = sphere(2000)\n\n\nA = gaussian_kernel(X,kernel_type = \"adaptive\", k = 10, anisotropic_density_normalization = 1, threshold_for_small_values=1e-5)\n\n\nA\n\narray([[0.0008864 , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.00087881, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.00113487, ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.00083949, 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.00071587,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.0009776 ]])\n\n\n\nsum(A)\n\narray([0.01780748, 0.01605671, 0.01757066, ..., 0.01677266, 0.01705182,\n       0.01707113])\n\n\n\nplot_3d(X,A[0])\n\n\n\n\n\nsource\n\ncompute_anisotropic_diffusion_matrix_from_graph\n\n compute_anisotropic_diffusion_matrix_from_graph (A:numpy.ndarray,\n                                                  alpha:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nA\nndarray\nthe adjacency/affinity matrix of the graph\n\n\nalpha\nfloat\nthe anisotropic density normalization parameter\n\n\nReturns\nndarray\n\n\n\n\n\nsource\n\n\ncompute_anisotropic_affinities_from_graph\n\n compute_anisotropic_affinities_from_graph (A:numpy.ndarray, alpha:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nA\nndarray\nthe adjacency/affinity matrix of the graph\n\n\nalpha\nfloat\nthe anisotropic density normalization parameter\n\n\nReturns\nndarray\n\n\n\n\n\n!nbdev_export"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "diffusion_curvature",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "diffusion_curvature",
    "section": "Install",
    "text": "Install\npip install diffusion_curvature"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "diffusion_curvature",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  }
]