# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/1f_continuous_normalizing_flows.ipynb.

# %% auto 0
__all__ = ['ManifoldNeighborhoodDataset', 'dataloader_from_manifold_neighborhoods', 'FlowNet', 'NegativeLogLikelihood',
           'NegativeLogLikelihoodQuaUniform', 'GreatFlattener', 'neural_flattener']

# %% ../nbs/1f_continuous_normalizing_flows.ipynb 7
from sklearn.decomposition import PCA
import torch, torch.nn as nn, torch.utils.data as data, torch.nn.functional as F, torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
import numpy as np
class ManifoldNeighborhoodDataset(data.Dataset):
    def __init__(self, X, n_neighbors, intrinsic_dimension):
        """
        Creates a dataset composed of the specified neighbors of the index point.
        """
        self.n_neighbors = n_neighbors
        self.X = torch.tensor(X).float()
        self.intrinsic_dimension = intrinsic_dimension
        self.scaler = MinMaxScaler(feature_range=(-1,1))
        
    def __len__(self):
        return int(self.n_neighbors)
    def __getitem__(self, idx):
        central_point = self.X[idx]
        # TODO: We presently assume that these neighborhoods are small enough that euclidean similarities are accurate
        distances_to_central_point = torch.linalg.norm(self.X - central_point, axis=1)
        neighbor_idxs = np.argsort(distances_to_central_point)[:self.n_neighbors]
        ambient_points = self.X[neighbor_idxs]
        pca = PCA(n_components=self.intrinsic_dimension)
        pca_points = pca.fit_transform(ambient_points.numpy())
        pca_points = self.scaler.fit_transform(pca_points)
        pca_points = torch.Tensor(pca_points)
        # scale pca'd points to unit (hyper)cube
        return ambient_points, pca_points
    
        

# %% ../nbs/1f_continuous_normalizing_flows.ipynb 8
def dataloader_from_manifold_neighborhoods(X, n_neighbors):
    ds = ManifoldNeighborhoodDataset(X, intrinsic_dimension=2, n_neighbors=n_neighbors)
    dataloader = data.DataLoader(ds, batch_size=None,shuffle=False)
    return dataloader

# %% ../nbs/1f_continuous_normalizing_flows.ipynb 21
class FlowNet(nn.Module):
    def __init__(self, dimension ,activation='CELU'):
        super().__init__()
        act_fn = getattr(nn, activation)
        self.sigmoid = nn.Sigmoid()
        self.act_fn = act_fn
        
        self.seq = nn.Sequential(
            nn.Linear(dimension,64),
            act_fn(),

            nn.Linear(64,128),
            act_fn(),
            
            nn.Linear(128, 128),
            act_fn(),
                        
            nn.Linear(128, 64),
            act_fn(),
            
            nn.Linear(64, 32),
            act_fn(),

            nn.Linear(32, 16),
            act_fn(),

            nn.Linear(16, dimension),
            act_fn(),

            nn.Linear(dimension,dimension),
        )

    def forward(self, x):
        velocities = self.seq(x)
        # filter velocities outside the unit cube to zero
        filter = torch.sigmoid(100*(-x+0.9))*torch.sigmoid(100*(x+0.9))
        return velocities*filter

# %% ../nbs/1f_continuous_normalizing_flows.ipynb 23
class NegativeLogLikelihood(nn.Module):    
    def __init__(self, model:nn.Module):
        super().__init__()
        self.model = model
    def __call__(self, transformed_points, J, prior):
        # logp(z_S) = logp(z_0) - \int_0^S trJ
        log_prob = prior.log_prob(transformed_points)[:,0] - J         
        loss = -torch.mean(log_prob)
        return loss
class NegativeLogLikelihoodQuaUniform(nn.Module):
    def __init__(self, model:nn.Module):
        super().__init__()
        self.model = model
    def __call__(self, transformed_points, J, prior):
        # logp(z_S) = logp(z_0) - \int_0^S trJ
        log_prob = prior.log_prob(transformed_points)
        # the uniform's prior function returns probs by dimension. We
        log_prob_z0 = torch.sum(log_prob,dim=1)
        log_prob_z1 = log_prob_z0 - J
        # Because we're comparing to the uniform distribution, the probs are all uniform - so this equates to minimizing the stretching within the domain, i.e.
        # penalizing the jacobian (which appears in the first column) to be as small as possible.           
        loss = -torch.mean(log_prob_z1)
        return loss

# %% ../nbs/1f_continuous_normalizing_flows.ipynb 27
import pytorch_lightning as pl
from torchdyn.nn import Augmenter
from torchdyn.models import CNF

class GreatFlattener(pl.LightningModule):
    def __init__(
        self, 
        model:nn.Module,        
        # train_loader:data.DataLoader,
        input_dim:int,
        target_dim:int,
        device,
    ):
        super().__init__()
        # NOTE: model here is the Neural ODE
        self.model = model.to(device)
        self.input_dim = input_dim #train_loader.dataset.X.shape[1]
        self.dim_of_target_distribution = target_dim #train_loader.dataset.intrinsic_dimension
        
        # The steps to integrate over
        self.t_span = torch.linspace(0, 1, 5).to(device)
        
        # Our dataset
        # self.train_loader = train_loader
        # The distribution from which we are sampling to generate points
        self.prior = torch.distributions.uniform.Uniform(-torch.ones(input_dim).to(device)-0.01, torch.ones(input_dim).to(device)+0.01)
        # self.prior = torch.distributions.MultivariateNormal(torch.zeros(input_dim), torch.eye(input_dim))
        self.loss = NegativeLogLikelihoodQuaUniform(self.model).to(device)
        self.outside_bounds_penalty = 0
        self.augmenter = Augmenter(augment_idx=1, augment_dims=1).to(device) # used, before calling the CNF, to add an extra column which CNF can fill with the jacobian

    # def train_dataloader(self):
    #     return self.train_loader
    
    def sample(self, shape):
        z = self.prior.sample(sample_shape=shape).to(torch.float32).to(self.device)
        # pad with zeros to extra dimensions
        # z = torch.hstack([z,torch.zeros(len(z),self.input_dim-self.dim_of_target_distribution)])
        return z
        
    def forward(self, points, t_span=None, return_trajectory = False):
        if t_span is None:
            t_span = self.t_span
        points = points[0]
        # Add extra column of zeros, to be filled by the jacobian's trace by the CNF
        points_with_extra_dim = self.augmenter(points)
        # Pass prepared data into the Neural ODE and integrate through time
        t_eval, trajectory = self.model(points_with_extra_dim, t_span) 
        # get last component of trajectory and separate points from the Jacobian diagonal
        last_stop = trajectory[-1]
        J = last_stop[:,0]
        transformed_points = last_stop[:,1:]
        
        # If points have flowed outside of the support of the uniform distribution, trap them and bring them back
        # Trapping them, unfortunately, isn't differentiable. I'll instead scale the points along each dimension
        # for i in range(transformed_points.shape[1]):
        #     transformed_points[:,i][transformed_points[:,i]>1] = 1
        #     transformed_points[:,i][transformed_points[:,i]<-1] = -1
        # for i in range(transformed_points.shape[1]):
        #     transformed_points[:,i]  = transformed_points[:,i]/torch.max(transformed_points[:,i])

        # give huge penalty for leaving the support of the uniform distribution.
        distance_from_box = torch.max(torch.abs(transformed_points), torch.ones_like(transformed_points)) - torch.ones_like(transformed_points) 
        self.outside_bounds_penalty = torch.exp(torch.sum(distance_from_box))
        signed_distance_from_box = distance_from_box * torch.sign(transformed_points)

        # normalize points back in box
        # transformed_points = transformed_points - signed_distance_from_box

        if return_trajectory:
            return trajectory
        else:
            return transformed_points, J
    
    def training_step(self, points):    
        transformed_points, J = self.forward(points)
        loss = self.loss(transformed_points, J, self.prior)
        loss = loss # + self.outside_bounds_penalty
        self.log('loss', loss, prog_bar=True)
        self.log('outside box penalty', self.outside_bounds_penalty, prog_bar=True)
        return {'loss': loss}  
        
    @torch.no_grad()
    def generate_data(self, n_points):
        # Goes through the ODE in reverse
        # NOTE: regardless of how you implemented model this function _should_ work!

        points = [self.sample(torch.Size([n_points]))] # enclose in [] to match the form provided by the dataloader
        # integrating from 1 to 0
        reverse_t_span = torch.linspace(1, 0, 2)
        new_data, J = self.forward(points, t_span=reverse_t_span)
        return new_data
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.99)
        return [optimizer], [scheduler]
    


# %% ../nbs/1f_continuous_normalizing_flows.ipynb 43
from torchdyn.nn import Augmenter
import pytorch_lightning as pl
import torch
from torchdyn.models import CNF, NeuralODE

class neural_flattener():
    def __init__(self, device, max_epochs=1000):
        self.device = device
        self.max_epochs = max_epochs

    def fit_transform(self, X):
        # create dataloader from input tensor
        X = torch.Tensor(X).float().to(self.device)
        trainset = data.TensorDataset(X)
        trainloader = data.DataLoader(trainset, batch_size=len(X), shuffle=True)
        # define flows and ode, initialize model
        dim = X.shape[1]
        trainable_flows = FlowNet(dimension=dim).to(self.device)
        flow = CNF(trainable_flows).to(self.device)
        self.ode = NeuralODE(flow, sensitivity='adjoint', solver='dopri5').to(self.device)
        # create our PyLightning learner to handle training steps
        self.flattener = GreatFlattener(self.ode, input_dim=dim, target_dim=dim, device=self.device)
        self.flattener = self.flattener.to(self.device)
        # train 
        self.trainer = pl.Trainer(
            max_epochs=self.max_epochs,
            
            # NOTE: gradient clipping can help prevent exploding gradients
            gradient_clip_val=100,
            gradient_clip_algorithm='value',    
            log_every_n_steps=5,
            
            # NOTE: this should match your device i.e. if you set cuda above, this should be cuda. 
            # Otherwise it should be cpu. 
            accelerator="cuda",    
            
            # NOTE: you can set the maximum time you want to train your model
            max_time={'minutes': 5},    

            # NOTE: setting this to true will save your model every so often
            enable_checkpointing=False,
            accumulate_grad_batches=2
        )
        self.trainer.fit(self.flattener, trainloader)
        # afterwards, take the transformed data
        self.flattener = self.flattener.to(self.device)
        transformed_data, J = self.flattener([X], t_span=torch.linspace(0,1,2).to(self.device))
        return transformed_data.cpu().detach().numpy()
